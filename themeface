#!/usr/bin/env python

import face_recognition
import getopt
import glob
import os.path
import signal
import sys
from sys import stdout as out
from timeit import default_timer as now

from pprint import pprint as pp

opts, args = getopt.getopt(sys.argv[1:], "vc:r:", ["camera=", "rotate="])
show_video = False
camera_index = None
rotation = 0
for opt, arg in opts:
    if opt == "-v":
        show_video = True
    elif opt in ["-c", "--camera"]:
        camera_index = int(arg)
    elif opt in ["-r", "--rotate"]:
        rotation = int(arg)

print("-> Starting Camera... ")
try:
    import camera.pi as camera
    print("Raspberry Pi Camera")
except:
    try:
        import camera.opencv as camera
        import cv2
        have_opencv = True
        print("Webcam via OpenCV")
    except ImportError:
        print("Cannot load any camera. Exiting.")
        exit(2)

if rotation != 0:
    print("Rotating image {} degrees clockwise.".format(rotation))

if camera_index is None:
    print("Default camera.")
    camera_index = 0
else:
    print("Camera index {}.".format(camera_index))

camera = camera.Camera((320, 240), camera_index=camera_index)
print("Done.")

print("-> Loading training data... ")
source_files = glob.glob("source_images/*.*")
names = []
source_encodings = []
for filename in source_files:
    print(filename)
    image = face_recognition.load_image_file(filename)
    encoding = face_recognition.face_encodings(image)[0]
    names.append(os.path.basename(filename).split(".")[0])
    source_encodings.append(encoding)

print("Done.")
# Initialize some variables
face_locations = []
face_encodings = []
face_names = []
process_this_frame = True

# No, globals aren't the best, but neither is this script!
processing = True
def signal_handler(signal, frame):
    global processing
    processing = False
signal.signal(signal.SIGINT, signal_handler)

print("-> Processing video... ")
while processing:
    start_time = now()

    # Grab a single frame of video
    frame = camera.capture()

    if rotation > 0:
        frame = camera.rotate(frame, rotation=rotation)

    # Find all the faces and face encodings in the current frame of video
    face_locations = face_recognition.face_locations(frame)
    face_encodings = face_recognition.face_encodings(frame, face_locations)

    face_names = []
    for face_encoding in face_encodings:
        matches = face_recognition.compare_faces(source_encodings, face_encoding)
        face_names = face_names + [name for name, match in zip(names, matches) if match]

    out.write("\r\33[2K")
    out.write("Found {} face".format(len(face_locations)))
    if len(face_locations) == 1:
        out.write(": ")
    elif len(face_locations) > 0:
        out.write("s: ")
    else:
        out.write("s")
    out.write(", ".join(sorted(face_names)))
    out.write(" in {0:.2f}s.".format(now() - start_time))
    out.flush()

    if show_video and have_opencv:
        for (top, right, bottom, left), name in zip(face_locations, face_names):
            cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)
            font = cv2.FONT_HERSHEY_DUPLEX
            cv2.putText(frame, name, (left + 6, bottom - 6), font, 0.5, (255, 255, 255), 1)

        cv2.imshow('Video', frame)
        cv2.waitKey(1)


camera.release()
if have_opencv:
    cv2.destroyAllWindows()
print("\nDone.")
